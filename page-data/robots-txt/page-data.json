{
    "componentChunkName": "component---src-templates-detail-template-jsx",
    "path": "/robots-txt/",
    "result": {"pageContext":{"previous":null,"node":{"html":"<p>웹사이트를 개발하고 배포하다 보면 한 번쯤 robots.txt라는 파일에 대해서 듣게 됩니다.\n저도 처음에는 검색 엔진이 크롤링할 때 필요한 정보를 제공해주는 수단으로 이해했었죠.</p>\n<p>하지만 개인 블로그를 운영하면서 느낀 건, 이 작고 단순한 파일이 생각보다 많은 역할을 한다는 겁니다.\n검색 결과에 어떤 페이지가 보이고 안 보이는지, 트래픽 낭비를 줄일 수 있는지 등, 의외로 중요한 결정들이 이 한 장짜리 텍스트 파일에 달려 있습니다.</p>\n<p>특히 요즘처럼 데이터 학습을 위해서 AI 크롤러가 우후죽순 등장하는 시대에는, 단순한 SEO 도구를 넘어 창작자의 콘텐츠 보호 수단으로 robots.txt를 다시 바라보게 됩니다.</p>\n<p>이 글에서는 robots.txt의 기본 개념부터 문법, 한계, 그리고 AI 시대에 맞는 전략적 사용법까지 정리해보려고 합니다.\n저처럼 개인 블로그를 운영하시는 분들께 특히 도움이 되길 바랍니다.</p>\n<h2 id=\"robotstxt란\" style=\"position:relative;\"><a href=\"#robotstxt%EB%9E%80\" aria-label=\"robotstxt란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>robots.txt란?</h2>\n<p>웹사이트 루트(예: <code>https://example.com/robots.txt</code>)에 위치한 이 작은 텍스트 파일 하나가, 사실 검색엔진 크롤러에게 보내는 **“여기서부터 여기까지만 와 주세요”**라는 안내 표시입니다.\n개발자로서 보면 “크롤러는 어디까지 와서 무엇을 훑어야 하나?”라는 질문에 답해주는 일종의 크롤링 안내서이죠.</p>\n<p>기본적으로, 검색엔진 봇이 처음 사이트에 접속하면 먼저 /robots.txt 파일을 찾아 읽습니다. 그 안에서 User‑agent(어떤 봇인지), Disallow(접근 금지 경로), Allow(접근 허용 경로), Sitemap(사이트맵 위치) 등의 토큰이 보이면, 해당 규칙을 해석해서 다음 동작을 결정하죠.</p>\n<p>예컨대:</p>\n<p>User‑agent: *\nDisallow: /drafts/</p>\n<p>이렇게 지정했다면, 모든 봇에게 /drafts/ 이하 경로는 “크롤링하지 말아 주세요”라는 신호입니다.</p>\n<p>개발자 관점에서 주의해야 할 것은: “크롤링 제한”이지 “검색엔진 결과에서 완전히 사라짐”은 아니다라는 점입니다. 즉, 크롤링을 막아도 인덱스에 남을 가능성이 있고, 반대로 noindex 메타 태그를 써도 크롤링은 허용돼 있을 수 있습니다. 이 둘을 혼동하면 “왜 내 글이 검색에 안 나와?” 혹은 “왜 숨겼는데 노출돼?”라는 미스터리를 만나게 될 수 있어요.</p>\n<h2 id=\"robotstxt의-문법\" style=\"position:relative;\"><a href=\"#robotstxt%EC%9D%98-%EB%AC%B8%EB%B2%95\" aria-label=\"robotstxt의 문법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>robots.txt의 문법</h2>\n<p>robots.txt 파일은 복잡해볼 수도 있지만 사실 문법은 사실 아주 간단합니다.\n5가지 지시문(directive)으로 이루어져있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">User‑agent: &lt;봇이름 또는 *></code></pre></div>\n<p>어떤 크롤러(봇)에게 규칙을 적용할지 지정합니다. *은 모든 봇 대상.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Disallow: &lt;경로 또는 *></code></pre></div>\n<p>해당 봇이 접근하지 말아야 할 경로를 지정합니다. 루트(/)를 적으면 사이트 전체 차단이 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Allow: &lt;경로></code></pre></div>\n<p>Disallow에 의해 차단된 범위 안에서 예외적으로 허용할 때 쓰입니다 (주로 Googlebot의 고급 규칙 등).</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Sitemap: &lt;URL></code></pre></div>\n<p>사이트맵 XML 파일 위치를 알려줘서 크롤러가 구조를 좀 더 효율적으로 이해하도록 돕습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Crawl‑delay: &lt;초 수></code></pre></div>\n<p>(지원하는 봇에 한해) 동일한 서버에 대한 요청 빈도를 제한할 수 있는 옵션입니다.</p>\n<p>예를 들어, 다음과 같은 <code>robots.txt</code> 파일을 함께 해석해볼께요?</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">User‑agent: Googlebot\nDisallow: /private/\nAllow: /\n\nSitemap: https://www.example.com/sitemap.xml</code></pre></div>\n<p>이 설정은 구글 검색 엔진의 크롤러인 Googlebot을 상대로하고 있으며,\n<code>/private/</code> 경로를 제외한 모든 경로의 크롤링을 허용한다는 의미입니다.</p>\n<p>또한 사이트맵 위치도 제공하고 있습니다.</p>\n<p>하나의 <code>robots.tx</code> 파일에 여러 규칙이 있으면 더 구체적인 규칙이 우선됩니다.\n예를 들어, <code>User-agent: *</code>에 대한 한 규칙보다 <code>User-agent: Googlebot</code>에 대한 규칙이 우선합니다.</p>\n<h2 id=\"user-agent의-종류\" style=\"position:relative;\"><a href=\"#user-agent%EC%9D%98-%EC%A2%85%EB%A5%98\" aria-label=\"user agent의 종류 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>User Agent의 종류</h2>\n<p>과거에는 우리의 웹사이트를 크롤링하는 User Agent가 주로 검색 엔진이었습니다.\n국내외 대표적인 검색 엔진의 크롤러의 이름을 정리해보면 다음과 같습니다.</p>\n<table>\n<thead>\n<tr>\n<th>회사</th>\n<th>User-agent 이름</th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Google</td>\n<td><code>Googlebot</code></td>\n<td>일반 웹 검색</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Googlebot-Image</code></td>\n<td>이미지 검색</td>\n</tr>\n<tr>\n<td></td>\n<td><code>Googlebot-News</code></td>\n<td>뉴스 검색</td>\n</tr>\n<tr>\n<td>Bing (Microsoft)</td>\n<td><code>Bingbot</code></td>\n<td>Bing 검색</td>\n</tr>\n<tr>\n<td></td>\n<td><code>MSNBot</code></td>\n<td>과거 사용되던 크롤러</td>\n</tr>\n<tr>\n<td>Yahoo</td>\n<td><code>Slurp</code></td>\n<td>Yahoo 검색</td>\n</tr>\n<tr>\n<td>DuckDuckGo</td>\n<td><code>DuckDuckBot</code></td>\n<td>DuckDuckGo 검색</td>\n</tr>\n<tr>\n<td>Baidu (중국)</td>\n<td><code>Baiduspider</code></td>\n<td>바이두 검색</td>\n</tr>\n<tr>\n<td>Yandex (러시아)</td>\n<td><code>YandexBot</code></td>\n<td>얀덱스 검색</td>\n</tr>\n<tr>\n<td>Naver (한국)</td>\n<td><code>Yeti</code></td>\n<td>네이버 검색</td>\n</tr>\n<tr>\n<td>Daum (한국)</td>\n<td><code>Daumoa</code></td>\n<td>다음/카카오 검색</td>\n</tr>\n</tbody>\n</table>\n<p>최근에는 데이터 학습을 위해서 AI 업체에서도 열심히 전세계의 웹사이트를 크롤링하고 있습니다.\n대표적인 AI 업체의 크롤러 이름은 다음과 같습니다.</p>\n<table>\n<thead>\n<tr>\n<th>회사</th>\n<th>User-agent 이름</th>\n<th>설명</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>OpenAI</td>\n<td><code>GPTBot</code></td>\n<td>ChatGPT 학습용 크롤러</td>\n</tr>\n<tr>\n<td></td>\n<td><code>ChatGPT-User</code></td>\n<td>사용자의 실시간 검색 요청 기반 수집</td>\n</tr>\n<tr>\n<td>Anthropic</td>\n<td><code>AnthropicBot</code></td>\n<td>Claude AI 학습용</td>\n</tr>\n<tr>\n<td>Perplexity</td>\n<td><code>PerplexityBot</code></td>\n<td>Perplexity AI 크롤러</td>\n</tr>\n<tr>\n<td>Google AI</td>\n<td><code>Google-Extended</code></td>\n<td>Gemini(Bard) 학습 데이터 수집 제어용</td>\n</tr>\n<tr>\n<td>Meta (페이스북)</td>\n<td><code>facebookexternalhit</code></td>\n<td>링크 미리보기용 (학습 목적 아님)</td>\n</tr>\n<tr>\n<td>Amazon</td>\n<td><code>Amazonbot</code></td>\n<td>Alexa 및 AI 서비스용 가능성 있음</td>\n</tr>\n<tr>\n<td>Common Crawl</td>\n<td><code>CCBot</code></td>\n<td>공개 크롤링 데이터 (AI 학습에 자주 활용됨)</td>\n</tr>\n<tr>\n<td>ByteDance (TikTok)</td>\n<td><code>Bytespider</code></td>\n<td>ByteDance AI 학습용 (중국)</td>\n</tr>\n<tr>\n<td>Apple</td>\n<td><code>Applebot</code></td>\n<td>Siri, Spotlight 등</td>\n</tr>\n<tr>\n<td>You.com</td>\n<td><code>youBot</code></td>\n<td>You.com 검색 및 AI 크롤러</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"robotstxt의-한계\" style=\"position:relative;\"><a href=\"#robotstxt%EC%9D%98-%ED%95%9C%EA%B3%84\" aria-label=\"robotstxt의 한계 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>robots.txt의 한계</h2>\n<p><code>robots.txt</code>는 강제성이 있는 법률 조항이 아니라 자율적으로 따르는 약속입니다.\n대부분의 검색 엔진 봇은 이 신호를 존중하지만, 악성 크롤러나 무차별 스크래퍼(scraper)에게는 무시당할 수 있어요.\n따라서 보안 수단으로 <code>robots.txt</code>를 여기는 것은 위함합니다.</p>\n<p>또한 <code>robots.txt</code>을 통해 크롤링을 막았다고 해서 자동으로 검색 결과에서 해당 페이지가 사라지는 것은 아닙니다.\n실제로 Disallow로 막인 페이지가, 다른 외부 링크 등을 통해 검색 엔진 인덱스에 남아있을 수 있어요.</p>\n<p>반대로 noindex 메타 태그는 인덱스에서 제외하라는 강한 신호지만, 그것을 봇이 읽기 위해서는 먼저 그 페이지에 접근해야 하므로,\n만약 robots.txt로 접근까지 막았다면 noindex가 작동하지 않습니다.</p>\n<p>정리하자면,</p>\n<ul>\n<li>Disallow → 크롤링 차단</li>\n<li>noindex → 인덱스에서 제거 요청\n이 둘은 목적이 다르고, 서로 보완적으로 사용하는 것이 바람직합니다.</li>\n</ul>\n<p>또 하나 주의사항: robots.txt는 인증(로그인)된 사용자 전용 콘텐츠를 숨기기 위한 용도로는 적합하지 않습니다.\n인증 레이어 없이 공개되어 있는 상태라면 단순히 Disallow만으로는 봇이 접근을 막지 못할 수 있어요.\n이럴 땐 서버 측에서 인증 혹은 nofollow/noindex 등의 추가 조치가 필요합니다.</p>\n<h2 id=\"ai-시대에-robotstxt\" style=\"position:relative;\"><a href=\"#ai-%EC%8B%9C%EB%8C%80%EC%97%90-robotstxt\" aria-label=\"ai 시대에 robotstxt permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AI 시대에 robots.txt</h2>\n<p>최근 들어 AI 학습용 크롤러들이 활발해지면서, <code>robots.txt</code>를 그저 SEO 차원의 도구로만 보는 시대는 지나가고 있습니다.\n예컨대 User Agent가 <code>GPTBot</code>, <code>AnthropicBot</code>, <code>PerplexityBot</code> 등과 같은 이름의 봇들이 실제로 웹을 돌아다니며 학습 데이터를 수집하고 있고, 일부 사이트 운영자는 이들에 대해 별도의 규칙을 설정하는 일이 늘고 있어요.</p>\n<p>회사에서든 개인적으로 웹사이트를 운영자하고 계시다면 다음과 같은 부분을 고려해보실 필요가 있습니다.</p>\n<ul>\n<li>AI 학습용 크롤러의 접근을 허용할 것인가, 차단할 것인가?</li>\n<li>콘텐츠(특히 기술 블로그나 전문 아티클)를 공개적으로 AI 학습에 활용되더라도 괜찮다는 입장이라면 허용 전략을, 내 글을 학습 데이터로 쓰는 건 원치 않는다면 차단 전략을 생각해야 합니다.</li>\n<li>기존 검색엔진 + AI 봇을 구분하는 규칙 구성\n예컨대:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"txt\"><pre class=\"language-txt\"><code class=\"language-txt\">User‑agent: *\nDisallow:\n\nUser‑agent: GPTBot\nDisallow: /\n\nUser‑agent: AnthropicBot\nDisallow: /</code></pre></div>\n<p>이렇게 하면 다른 크롤러는 허용하되, 특정 AI 봇만 차단하는 균형적 접근이 가능해집니다.</p>\n<h2 id=\"content-signal의-등장\" style=\"position:relative;\"><a href=\"#content-signal%EC%9D%98-%EB%93%B1%EC%9E%A5\" aria-label=\"content signal의 등장 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Content Signal의 등장</h2>\n<p>최근에는 기존의 <code>User-agent</code>와 <code>Disallow</code>만으로는 부족하다는 인식이 생기면서, 콘텐츠의 사용 목적까지 명시하는 시도가 등장하고 있습니다.\n대표적인 예가 바로 Cloudflare에서 제안한 <code>Content-Signal</code>이라는 지시문입니다.</p>\n<p><a href=\"https://blog.cloudflare.com/content-signals-policy/\">https://blog.cloudflare.com/content-signals-policy/</a></p>\n<div class=\"gatsby-highlight\" data-language=\"txt\"><pre class=\"language-txt\"><code class=\"language-txt\">User-Agent: *\nContent-Signal: search=yes, ai-train=no\nAllow: /</code></pre></div>\n<p>위 설정은 모든 봇에게 접근을 허용하고 있지만,\n이 콘텐츠는 검색 색인용으로는 사용 가능하지만 AI 모델 학습용으로는 사용하지 말아달라는 의사 표현입니다.</p>\n<p>즉, 이 지시문은 크롤링 허용 여부를 넘어서, 크롤링된 콘텐츠가 어떻게 활용되기를 원하는지에 대한 운영자의 선호를 명시하는 것이죠.</p>\n<p>하지만 아쉽게도 아직까지 모든 크롤링 봇이 <code>Content-Signal</code> 지시문을 따라고 있지는 않은 상황입니다.\nCloudflare가 2024년 제안한 비교적 새로운 지시문으로, 공식 표준(RFC)에 포함된 것은 아닙니다.</p>\n<p>하지만 점점 더 많은 사이트와 크롤러가 이 신호에 주목하고 있고, 향후 업계 표준으로 자리 잡을 가능성도 있습니다.\n실제로 Perplexity, Brave 등 일부 기업은 Content-Signal을 일부 지원한다고 밝히기도 했습니다.</p>\n<h2 id=\"robotstxt-예시\" style=\"position:relative;\"><a href=\"#robotstxt-%EC%98%88%EC%8B%9C\" aria-label=\"robotstxt 예시 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>robots.txt 예시</h2>\n<p>이제 “내 블로그는 어떻게 설정할까?” 고민하는 운영자들을 위해 세 가지 전략 예시를 들어보겠습니다.</p>\n<p>🟩 개방적 전략 (모두 허용)</p>\n<div class=\"gatsby-highlight\" data-language=\"txt\"><pre class=\"language-txt\"><code class=\"language-txt\">User‑agent: \\*\nDisallow:</code></pre></div>\n<p>AI 시대 전부터 오픈 웹의 가치와 철학을 지향하는 웹사이트에서 많이 사용하던 설정입니다.\n자유롭게 인덱싱과 크롤링을 허용하기 때문에 유입 트래픽을 최대화할 수 있는 장점이 있습니다.\n하지만 AI 학습 차원에서 콘텐츠가 대량 수집될 가능성 있어서 주의가 필요한 설정이기도 합니다.</p>\n<p>🟧 균형적 전략 (검색 허용, AI 학습 차단)</p>\n<div class=\"gatsby-highlight\" data-language=\"txt\"><pre class=\"language-txt\"><code class=\"language-txt\">User-Agent: *\nContent-Signal: search=yes, ai-train=no\nAllow: /\n\nUser‑agent: GPTBot\nDisallow: /\nUser‑agent: AnthropicBot\nDisallow: /</code></pre></div>\n<p>검색 엔진을 통합 유입은 중요하지만 AI 학습에 콘텐츠가 활용되는 것은 원치 않는 개인 블로그나 콘텐츠 사이트에서 많이 쓰이는 설정입니다.\n위에서 다룬 <code>Content-Signal</code> 지시문을 통해서 AI 모델 학습용으로 콘텐츠가 활용되는 것을 명시적으로 거부하고 있습니다.</p>\n<p>🟥 보수적 전략 (AI 포함 전면 차단)</p>\n<div class=\"gatsby-highlight\" data-language=\"txt\"><pre class=\"language-txt\"><code class=\"language-txt\">User‑agent: *\nDisallow: /\n\nUser‑agent: GPTBot\nDisallow: /\nUser‑agent: AnthropicBot\nDisallow: /</code></pre></div>\n<p>보안이 중요하여 콘텐츠가 전혀 인덱싱 크롤링되기를 바라지 않는 웹사이트에서는 사용하는 설정입니다.\n불필요한 크롤러 요청과 트래픽 낭비 최소화할 수 있지만 검색 엔진 웹사이트가 전혀 노출되지 않을 수 있기 때문에 트래픽 유입에는 불리합니다.</p>\n<h2 id=\"마치며\" style=\"position:relative;\"><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\" aria-label=\"마치며 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>마치며</h2>\n<p>웹사이트를 운영하면서 <code>robots.txt</code>를 대수롭지 않은 설정 파일로 생각하기 쉽지만, 사실은 검색엔진 크롤링 + AI 학습 + 사이트 철학이 맞물리는 접점에 놓여있는 설정입니다.</p>\n<p>작은 설정 하나로도 검색 결과가 바뀌고, 트래픽 흐름이 달라질 수 있다는 점 명심하세요.</p>","timeToRead":7,"fields":{"slug":"/robots-txt/","tags":["Web","SEO","robots"]},"frontmatter":{"title":"검색엔진과 AI 크롤러를 안내하는 robots.txt 사용법","date":"Oct 8, 2025"}},"next":{"fields":{"slug":"/at-least-offer-interviews-here/"},"frontmatter":{"title":"적어도 여기는 면접 기회는 주더라고요"}}}},
    "staticQueryHashes": ["2168229929","2362167539","3056348342"]}